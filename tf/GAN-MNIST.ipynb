{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN-MNIST\n",
    "https://github.com/taki0112/GAN-Tensorflow/blob/master/Vanilla_GAN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/abs/1406.2661\n",
    "# Generative Adversarial Network(GAN)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Hyper parameter\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "\n",
    "# The amount of noise to use as input to the generator\n",
    "n_noise = 128\n",
    "\n",
    "# Since GAN is also an unsupervised learning, it does not use Y like Autoencoder.\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "\n",
    "# Use noise Z as input value.\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "def generator(noise_z) :\n",
    "    with tf.variable_scope('generator') :\n",
    "        hidden = tf.layers.dense(inputs=noise_z, units=n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(inputs=hidden, units=n_input, activation=tf.nn.sigmoid)\n",
    "\n",
    "    return output\n",
    "\n",
    "def discriminator(inputs, reuse=None) :\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        # In order to make the variables of the models that discriminate the actual image from the images generated by the noise the same,\n",
    "        # Reuse the previously used variables.\n",
    "\n",
    "        if reuse :\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        hidden = tf.layers.dense(inputs=inputs, units=n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(inputs=hidden, units=1, activation=tf.nn.sigmoid)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_noise(batch_size, n_noise) :\n",
    "    return np.random.normal(size=(batch_size, n_noise))\n",
    "\n",
    "\n",
    "# Generate random images using noise\n",
    "G = generator(Z)\n",
    "\n",
    "# Returns the value determined using the real image.\n",
    "D_real = discriminator(X)\n",
    "\n",
    "# Returns a value that determines whether the image created using noise is a real image.\n",
    "D_gene = discriminator(G, reuse=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "According to the paper, optimization of the GAN model maximizes loss_G and loss_D.\n",
    "We minimize the value of D_gene to maximize loss_D.\n",
    "\n",
    "This is because...\n",
    "When you insert the real image in the discriminator, it tries to have the maximum value as: tf.log (D_real) \n",
    "And the maximum value as: tf.log (1 - D_gene) even when you insert a fake image.\n",
    "\n",
    "This makes the discriminator learn the discriminator neural network so that the image produced by the generator is judged to be fake.\n",
    "\"\"\"\n",
    "\n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))\n",
    "tf.summary.scalar('loss_D', -loss_D)\n",
    "\n",
    "\"\"\"\n",
    "On the other hand, to maximize loss_G, we maximize the value of D_gene,\n",
    "It learns the generator neural network so that when the false image is inserted, the discriminator judges that the image is as real as possible.\n",
    "\n",
    "In the paper, we find a generator that minimizes to a formula such as loss_D,\n",
    "This is the same as maximizing the D_gene value, so you can use: loss_G = tf.reduce_mean(tf.log(D_gene))\n",
    "\"\"\"\n",
    "\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))\n",
    "tf.summary.scalar('loss_G', -loss_G)\n",
    "\n",
    "# If you want to see another loss function, see the following link.\n",
    "# http://bamos.github.io/2016/08/09/deep-completion/\n",
    "\n",
    "# When loss_D is obtained, only variables used in the generator neural network are used,\n",
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "\n",
    "# According to the GAN thesis formula, the loss should be maximized, but since the optimization function is used to minimize it, a negative sign is added to loss_D and loss_G to be optimized.\n",
    "train_D = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-loss_D, var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(-loss_G, var_list=vars_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss: 0.3292 G loss: 2.264\n",
      "Epoch: 0010 D loss: 0.2187 G loss: 2.866\n",
      "Epoch: 0020 D loss: 0.3228 G loss: 3.027\n",
      "Epoch: 0030 D loss: 0.2062 G loss: 3.516\n",
      "Epoch: 0040 D loss: 0.2656 G loss: 3.497\n",
      "Epoch: 0050 D loss: 0.2655 G loss: 3.787\n",
      "Epoch: 0060 D loss: 0.3973 G loss: 3.074\n",
      "Epoch: 0070 D loss: 0.2671 G loss: 3.037\n",
      "Epoch: 0080 D loss: 0.3456 G loss: 3.025\n",
      "Epoch: 0090 D loss: 0.5243 G loss: 2.503\n",
      "Optimized!\n"
     ]
    }
   ],
   "source": [
    "# Start training !\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "\n",
    "for epoch in range(total_epoch) :\n",
    "    for i in range(total_batch) :\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "\n",
    "        # It learns discriminator and generator neural network separately.\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                 feed_dict={X : batch_x, Z : noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                 feed_dict={Z : noise})\n",
    "    summary = sess.run(merged, feed_dict={X: batch_x, Z: noise})\n",
    "    writer.add_summary(summary, global_step=epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:', '%04d' % epoch,\n",
    "              'D loss: {:.4}'.format(-loss_val_D),\n",
    "              'G loss: {:.4}'.format(-loss_val_G))\n",
    "\n",
    "    # Create and save images periodically to see how learning is going\n",
    "    if epoch == 0 or epoch % 10 == 0 or epoch == total_epoch-1:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Z : noise})\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=sample_size, figsize=(sample_size, 1))\n",
    "\n",
    "        for i in range(sample_size) :\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i], (28,28)))\n",
    "\n",
    "        plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "print('Optimized!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
