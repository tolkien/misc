{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN (Generative Adversarial Network)\n",
    "- https://github.com/taki0112/GAN-Tensorflow\n",
    "- https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from skimage.io import imsave\n",
    "import os\n",
    "import shutil\n",
    "import gzip\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "\n",
    "img_size = img_height * img_width\n",
    "\n",
    "to_train = True\n",
    "to_restore = False\n",
    "output_path = \"samples\"\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "h1_size = 150\n",
    "h2_size = 300\n",
    "z_size = 100\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제너레이터 (G)\n",
    "\n",
    "def generator(z):\n",
    "    w1 = tf.Variable(tf.truncated_normal([z_size, h1_size], stddev=0.1), name = \"g_w1\", dtype=tf.float32)\n",
    "    b1 = tf.Variable(tf.zeros([h1_size]), name = \"g_b1\", dtype=tf.float32)\n",
    "    h1 = tf.nn.relu(tf.matmul(z,w1)+b1)\n",
    "    \n",
    "    w2 = tf.Variable(tf.truncated_normal([h1_size, h2_size], stddev=0.1), name = \"g_w2\", dtype=tf.float32)\n",
    "    b2 = tf.Variable(tf.zeros([h2_size]), name = \"g_b2\", dtype=tf.float32)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1,w2)+b2)\n",
    "\n",
    "    w3 = tf.Variable(tf.truncated_normal([h2_size, img_size], stddev=0.1), name = \"g_w3\", dtype=tf.float32)\n",
    "    b3 = tf.Variable(tf.zeros([img_size]), name = \"g_b3\", dtype=tf.float32)\n",
    "    h3 = tf.matmul(h2,w3) + b3    \n",
    "    x_generated = tf.nn.tanh(h3)\n",
    "    \n",
    "    g_params = [w1, b1, w2, b2, w3, b3]\n",
    "    return x_generated, g_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디스크리미네이터 (D)\n",
    "\n",
    "def discriminator(x_data, x_generated, keep_prob):\n",
    "    x_in = tf.concat([x_data, x_generated],0)\n",
    "    \n",
    "    w1 = tf.Variable(tf.truncated_normal([img_size, h2_size], stddev=0.1), name = \"d_w1\", dtype=tf.float32)\n",
    "    b1 = tf.Variable(tf.zeros([h2_size]), name = \"d_b1\", dtype=tf.float32)\n",
    "    h1 = tf.nn.dropout(tf.nn.relu(tf.matmul(x_in,w1)+b1), keep_prob=keep_prob)\n",
    "    \n",
    "    w2 = tf.Variable(tf.truncated_normal([h2_size, h1_size], stddev=0.1), name = \"d_w2\", dtype=tf.float32)\n",
    "    b2 = tf.Variable(tf.zeros([h1_size]), name = \"d_b2\", dtype=tf.float32)\n",
    "    h2 = tf.nn.dropout(tf.nn.relu(tf.matmul(h1,w2)+b2), keep_prob=keep_prob)\n",
    "\n",
    "    w3 = tf.Variable(tf.truncated_normal([h1_size, 1], stddev=0.1), name = \"d_w3\", dtype=tf.float32)\n",
    "    b3 = tf.Variable(tf.zeros([1]), name = \"d_b3\", dtype=tf.float32)\n",
    "    h3 = tf.matmul(h2,w3) + b3\n",
    "\n",
    "    y_data = tf.nn.sigmoid(tf.slice(h3, [0,0], [batch_size, -1], name=None))\n",
    "    y_generated = tf.nn.sigmoid(tf.slice(h3, [batch_size, 0], [-1,-1], name=None))\n",
    "    \n",
    "    d_params = [w1, b1, w2, b2, w3, b3]\n",
    "    return y_data, y_generated, d_params   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(batch_res, fname, grid_size=(8, 8), grid_pad=5):\n",
    "    batch_res = 0.5 * batch_res.reshape((batch_res.shape[0], img_height, img_width)) + 0.5\n",
    "    img_h, img_w = batch_res.shape[1], batch_res.shape[2]\n",
    "    grid_h = img_h * grid_size[0] + grid_pad * (grid_size[0] - 1)\n",
    "    grid_w = img_w * grid_size[1] + grid_pad * (grid_size[1] - 1)\n",
    "    img_grid = np.zeros((grid_h, grid_w), dtype=np.uint8)\n",
    "    for i, res in enumerate(batch_res):\n",
    "        if i >= grid_size[0] * grid_size[1]:\n",
    "            break\n",
    "        img = (res) * 255.\n",
    "        img = img.astype(np.uint8) \n",
    "        row = (i // grid_size[0]) * (img_h + grid_pad)\n",
    "        col = (i % grid_size[1]) * (img_w + grid_pad)\n",
    "        img_grid[row:row + img_h, col:col + img_w] = img\n",
    "    imsave(fname, img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():    \n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    \n",
    "    train_x, train_y = load_mnist(\"fashion_mnist\", kind='train')\n",
    "    size = train_x.shape[0]\n",
    "    x_data = tf.placeholder(tf.float32, [batch_size, img_size], name = \"x_data\")\n",
    "    z = tf.placeholder(tf.float32, [batch_size, z_size], name = 'z')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    \n",
    "    x_generated, g_params = generator(z)\n",
    "    y_data, y_generated, d_params = discriminator(x_data, x_generated, keep_prob)\n",
    "    \n",
    "    d_loss = - (tf.log(y_data) + tf.log(1 - y_generated))\n",
    "    g_loss = - tf.log(y_generated)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "\n",
    "    d_trainer = optimizer.minimize(d_loss, var_list=d_params)\n",
    "    g_trainer = optimizer.minimize(g_loss, var_list=g_params)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    if to_restore:\n",
    "        chkpt_fname = tf.train.latest_checkpoint(output_path)\n",
    "        saver.restore(sess, chkpt_fname)\n",
    "    else:\n",
    "        if os.path.exists(output_path):\n",
    "            shutil.rmtree(output_path)\n",
    "        os.mkdir(output_path)\n",
    "\n",
    "    z_sample_val = np.random.normal(0, 1, size=(batch_size, z_size)).astype(np.float32)\n",
    "\n",
    "    for i in range(sess.run(global_step), max_epoch):\n",
    "        if i % 50 == 0:\n",
    "            print(\"epoch:%s\" % (i))\n",
    "\n",
    "        for j in range(21870 // batch_size):\n",
    "\n",
    "            batch_end = j * batch_size + batch_size\n",
    "            if batch_end >= size:\n",
    "                batch_end = size - 1\n",
    "            x_value = train_x[ j * batch_size : batch_end ]\n",
    "            x_value = x_value / 255.\n",
    "            x_value = 2 * x_value - 1\n",
    "\n",
    "            z_value = np.random.normal(0, 1, size=(batch_size, z_size)).astype(np.float32)\n",
    "            sess.run(d_trainer,\n",
    "                     feed_dict={x_data: x_value, z: z_value, keep_prob: np.sum(0.7).astype(np.float32)})\n",
    "            if j % 1 == 0:\n",
    "                sess.run(g_trainer,\n",
    "                         feed_dict={x_data: x_value, z: z_value, keep_prob: np.sum(0.7).astype(np.float32)})\n",
    "        x_gen_val = sess.run(x_generated, feed_dict={z: z_sample_val})\n",
    "        if i % 10 == 0 or i == max_epoch-1:\n",
    "            show_result(x_gen_val, os.path.join(output_path, \"sample%s.jpg\" % i))\n",
    "        sess.run(tf.assign(global_step, i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "epoch:50\n",
      "epoch:100\n",
      "epoch:150\n",
      "epoch:200\n",
      "epoch:250\n",
      "epoch:300\n",
      "epoch:350\n",
      "epoch:400\n",
      "epoch:450\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
